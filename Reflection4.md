# Data Reflection 4

__Question: Provide three examples from Geoff West’s reading that illustrate how scale can be used to more fully understand human development as a complex and adapting social and economic system.  Additionally, what did Geoff West have to say about the use of theory and big data?  According to West, can a theory be relevant in the face of big data?  Provide an example.__


In order to explain the concept of scaling, West referred to biological term metabolic rate, which is the amount of energy needed per second to keep an organism alive. For example, a woman who weighs 120 pound typically needs 1,300 food calories per day to stay alive. Normally, people would consider her dog which weighs 60 pounds needs half of food calories which the woman needs for living. However, it turns out that the dog requires 880 food calories per day. The relation between body weight and food calories needed per day is a non-linear one, as the example showed. According to West, the non-linear relation is not only applicable to the two variables mentioned, but actually is a law which is existing in organisms and many other situations. Another example is about GDP of cities. While metropolitan Oklahoma City has a population of 12 million and GDP of about $60 billion, metropolitan Los Angeles, which is ten times larger than Oklahoma, has a GDP of more than $700 billion, which is 15% more than ten times of $60 million. The these two examples, the previous one is a sub-linear scaling, as the per capita gets smaller if weight gets larger; the latter example indicates a super-linear scaling, as the greater the population is, the more there is for per capita.

The cities are called complex because of this non-linear feature. Even though the city itself is an abstract concept, we are able to tell when we see one. You can say that the city is consists of buildings, roads and people in it, but in fact it is much more than that. The economic output, creative ideas, and culture are all created from interactions between inhabitants in the city, which is more than animals and plants in this city.

As a result, scalings should be considered during the process of city planning. Dirk Helbing, the director of the Institute for Transport and Economics at Dresden University of Technology in Germany, recruited his student Christian Kuhnert to investigate the relation between numbers of gasoline stations and city size. After plotting these two variables, it turns out that they follow a simple power rule which is a straight line with slope of 0.85. the numbers discovers how gasoline stations scale across the countries, which is a sub-linear scaling. To further explain the result, the greater the city size, the city which has double population of another city only need 85% more of gasoline stations, as one station can serve more people and earn more profits each month. In addition, other than gasoline stations, other infrastructures doing with transportation and supply networks obey the same 0.85 rule, and it is not only happening in Europe, but also global. Based on this rule,  some large cities might be able to reduce number of gasoline stations which are not efficient, consequently much energy and materials can be saved. Reduce amount of gasoline stations can save environment as well. Though cities and human development are complex, scaling is useful when we are trying to understand and investigate what is lying under them, just like the example above shows.

As for the relation between theory and big data, West’s idea is not too extreme. He considers it is better to augment traditional scientific models and theories with relevant data. Also, he does not think data itself can lead to any certain final conclusion, as base and direction of using data is previous success and failure produced by traditional methods. The discover of Higgs particle gives good example here. A machine called Large Hadron Collider (LHC) is used to measure collides between particles in a particular space. Amount of data produced by this machine is numerous, for about 600 million collisions per second. It is futile and inefficient to look through each of these collisions, and eventually Higgs relied on a conceptual framework and algorithm in order to focus on a specific subset, then leads to the discover of the particle.
